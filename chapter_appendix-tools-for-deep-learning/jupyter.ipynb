{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "OZiXKNNlPAHp",
   "metadata": {
    "id": "OZiXKNNlPAHp"
   },
   "source": [
    "## Problem statment: 70\n",
    "## Name:\n",
    "1. DEVKAR MANISH VIKRAM UJWALA | 2022jp15021 | Contribution : 100%\n",
    "\n",
    "\n",
    "## Dataset Used for Implementation\n",
    "\n",
    "**Title:** Heart Disease Dataset\n",
    "\n",
    "**Source:** Kaggle - https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset\n",
    "\n",
    "**Description:** This dataset contains medical attributes of patients and is used to predict the presence of heart disease. The dataset includes features such as age, sex, chest pain type, resting blood pressure, cholesterol levels, and other relevant medical indicators.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62912d82",
   "metadata": {},
   "source": [
    "# 1. Import Libraries/Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kP9OiTS2LzPv",
   "metadata": {
    "id": "kP9OiTS2LzPv"
   },
   "outputs": [],
   "source": [
    "# Import PySpark libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, isnan, when, isnull\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "# Import other useful libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create a Spark Session\n",
    "spark = SparkSession.builder.appName(\"HeartDisease_Prediction\").getOrCreate()\n",
    "\n",
    "# Display Spark Version\n",
    "print(\"PySpark Version:\", spark.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UAlkJpWAL9Fc",
   "metadata": {
    "id": "UAlkJpWAL9Fc"
   },
   "outputs": [],
   "source": [
    "# Load the dataset into PySpark DataFrame\n",
    "try:\n",
    "    df = spark.read.csv(\"heart.csv\", header=True, inferSchema=True)\n",
    "    print(\"Dataset loaded successfully from heart.csv\")\n",
    "except:\n",
    "    print(\"Please download the dataset from Kaggle and place it as 'heart.csv' in the current directory\")\n",
    "    print(\"\\nExpected dataset columns:\")\n",
    "    print(\"   age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpeak, slope, ca, thal, target\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sk4dzCQOMQse",
   "metadata": {
    "id": "sk4dzCQOMQse"
   },
   "outputs": [],
   "source": [
    "# Display basic dataset information\n",
    "if 'df' in locals():\n",
    "    print(\"Dataset Schema:\")\n",
    "    df.printSchema()\n",
    "    print(\"\\nDataset Shape (Rows, Columns):\", (df.count(), len(df.columns)))\n",
    "    print(\"\\nColumn Names:\")\n",
    "    print(df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a834fe2",
   "metadata": {},
   "source": [
    "# 2. Data Visualization and Exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ymNKs8fSMWSQ",
   "metadata": {
    "id": "ymNKs8fSMWSQ"
   },
   "outputs": [],
   "source": [
    "# 2a. Print first 5 rows for sanity check\n",
    "if 'df' in locals():\n",
    "    print(\"First 5 Rows of the Dataset:\")\n",
    "    df.show(5, truncate=False)\n",
    "\n",
    "    # Convert to Pandas for easier visualization\n",
    "    df_pd = df.toPandas()\n",
    "    print(\"\\nFirst 5 Rows (Pandas view):\")\n",
    "    print(df_pd.head())\n",
    "\n",
    "    # Identify target variable\n",
    "    print(\"\\nTarget Variable: 'target' (0 = No heart disease, 1 = Heart disease present)\")\n",
    "    print(\"Features present in the dataset:\")\n",
    "    for col_name in df.columns:\n",
    "        if col_name != 'target':\n",
    "            print(f\"   - {col_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9_Zbc08YMhF8",
   "metadata": {
    "id": "9_Zbc08YMhF8"
   },
   "outputs": [],
   "source": [
    "# 2b. Print dataset description and shape\n",
    "if 'df' in locals():\n",
    "    print(\"Dataset Shape:\")\n",
    "    num_rows = df.count()\n",
    "    num_cols = len(df.columns)\n",
    "    print(f\"   Rows: {num_rows}\")\n",
    "    print(f\"   Columns: {num_cols}\")\n",
    "\n",
    "    print(\"\\nDataset Description (Summary Statistics):\")\n",
    "    df_pd = df.toPandas()\n",
    "    print(df_pd.describe())\n",
    "\n",
    "    print(\"\\nData Types:\")\n",
    "    df.printSchema()\n",
    "\n",
    "    print(\"\\nMissing Values Check:\")\n",
    "    from pyspark.sql.functions import col, sum as spark_sum\n",
    "    missing_counts = df.select([spark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
    "    missing_counts.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hLOpVvJcMkBn",
   "metadata": {
    "id": "hLOpVvJcMkBn"
   },
   "outputs": [],
   "source": [
    "# 2c. Visualizations - Class Distribution\n",
    "if 'df' in locals():\n",
    "    df_pd = df.toPandas()\n",
    "\n",
    "    # Class distribution - Bar chart\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    class_counts = df_pd['target'].value_counts().sort_index()\n",
    "    plt.bar(['No Disease (0)', 'Heart Disease (1)'], class_counts.values, color=['skyblue', 'salmon'])\n",
    "    plt.title('Class Distribution - Heart Disease Dataset')\n",
    "    plt.xlabel('Target Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # Class distribution - Pie chart\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.pie(class_counts.values, labels=['No Disease (0)', 'Heart Disease (1)'],\n",
    "            autopct='%1.1f%%', colors=['skyblue', 'salmon'], startangle=90)\n",
    "    plt.title('Class Distribution - Percentage')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Class Distribution Counts:\")\n",
    "    print(class_counts)\n",
    "    print(f\"\\nClass Balance Ratio: {min(class_counts.values) / max(class_counts.values):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oLpQzJNfMxFF",
   "metadata": {
    "id": "oLpQzJNfMxFF"
   },
   "outputs": [],
   "source": [
    "# 2c. Visualizations - Feature Distributions (Histograms)\n",
    "if 'df' in locals():\n",
    "    df_pd = df.toPandas()\n",
    "\n",
    "    # Select numerical features for visualization\n",
    "    numerical_cols = df_pd.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if 'target' in numerical_cols:\n",
    "        numerical_cols.remove('target')\n",
    "\n",
    "    # Create histograms for numerical features\n",
    "    n_cols = 3\n",
    "    n_rows = (len(numerical_cols) + n_cols - 1) // n_cols\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4*n_rows))\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes] if n_rows == 1 else axes\n",
    "\n",
    "    for idx, col_name in enumerate(numerical_cols[:len(axes)]):\n",
    "        axes[idx].hist(df_pd[col_name].dropna(), bins=30, edgecolor='black', color='skyblue', alpha=0.7)\n",
    "        axes[idx].set_title(f'Distribution of {col_name}')\n",
    "        axes[idx].set_xlabel(col_name)\n",
    "        axes[idx].set_ylabel('Frequency')\n",
    "        axes[idx].grid(alpha=0.3)\n",
    "\n",
    "    # Hide empty subplots\n",
    "    for idx in range(len(numerical_cols), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fYRjS_6pM1LX",
   "metadata": {
    "id": "fYRjS_6pM1LX"
   },
   "outputs": [],
   "source": [
    "# 2c. Visualizations - Correlation Heatmap\n",
    "if 'df' in locals():\n",
    "    df_pd = df.toPandas()\n",
    "\n",
    "    # Calculate correlation matrix\n",
    "    numerical_cols = df_pd.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    corr_matrix = df_pd[numerical_cols].corr()\n",
    "\n",
    "    # Plot correlation heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "                square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "    plt.title('Correlation Heatmap - Heart Disease Dataset', fontsize=14, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Identify highly correlated features\n",
    "    print(\"Highly Correlated Feature Pairs (|correlation| > 0.5):\")\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            corr_val = corr_matrix.iloc[i, j]\n",
    "            if abs(corr_val) > 0.5:\n",
    "                high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_val))\n",
    "\n",
    "    if high_corr_pairs:\n",
    "        for feat1, feat2, corr_val in high_corr_pairs:\n",
    "            print(f\"   {feat1} <-> {feat2}: {corr_val:.3f}\")\n",
    "    else:\n",
    "        print(\"   No highly correlated pairs found (threshold: 0.5)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YsH0eY1pM4VI",
   "metadata": {
    "id": "YsH0eY1pM4VI"
   },
   "outputs": [],
   "source": [
    "# 2c. Visualizations - Box Plots for Outlier Detection\n",
    "if 'df' in locals():\n",
    "    df_pd = df.toPandas()\n",
    "\n",
    "    numerical_cols = df_pd.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if 'target' in numerical_cols:\n",
    "        numerical_cols.remove('target')\n",
    "\n",
    "    # Create box plots\n",
    "    n_cols = 3\n",
    "    n_rows = (len(numerical_cols) + n_cols - 1) // n_cols\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4*n_rows))\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes] if n_rows == 1 else axes\n",
    "\n",
    "    for idx, col_name in enumerate(numerical_cols[:len(axes)]):\n",
    "        axes[idx].boxplot(df_pd[col_name].dropna(), vert=True)\n",
    "        axes[idx].set_title(f'Box Plot - {col_name}')\n",
    "        axes[idx].set_ylabel('Value')\n",
    "        axes[idx].grid(alpha=0.3)\n",
    "\n",
    "    # Hide empty subplots\n",
    "    for idx in range(len(numerical_cols), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4385d49c",
   "metadata": {},
   "source": [
    "# 3. Data Pre-processing and Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47Dh8uEhM98h",
   "metadata": {
    "id": "47Dh8uEhM98h"
   },
   "outputs": [],
   "source": [
    "# 2d. Data Insights and Exploration\n",
    "if 'df' in locals():\n",
    "    df_pd = df.toPandas()\n",
    "\n",
    "    print(\"Data Insights Summary:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\n1. Dataset Overview:\")\n",
    "    print(f\"   - Total Records: {len(df_pd)}\")\n",
    "    print(f\"   - Total Features: {len(df_pd.columns) - 1}\")  # Excluding target\n",
    "    print(f\"   - Target Variable: 'target' (Binary Classification)\")\n",
    "\n",
    "    print(\"\\n2. Class Balance Analysis:\")\n",
    "    class_counts = df_pd['target'].value_counts().sort_index()\n",
    "    print(f\"   - Class 0 (No Disease): {class_counts[0]} ({class_counts[0]/len(df_pd)*100:.1f}%)\")\n",
    "    print(f\"   - Class 1 (Heart Disease): {class_counts[1]} ({class_counts[1]/len(df_pd)*100:.1f}%)\")\n",
    "    balance_ratio = min(class_counts.values) / max(class_counts.values)\n",
    "    if balance_ratio > 0.8:\n",
    "        print(f\"   - Dataset is balanced (ratio: {balance_ratio:.3f})\")\n",
    "    else:\n",
    "        print(f\"   - Dataset is imbalanced (ratio: {balance_ratio:.3f})\")\n",
    "\n",
    "    print(\"\\n3. Missing Values:\")\n",
    "    missing = df_pd.isnull().sum()\n",
    "    if missing.sum() == 0:\n",
    "        print(\"   No missing values found in the dataset\")\n",
    "    else:\n",
    "        print(\"   Missing values detected:\")\n",
    "        for col, count in missing[missing > 0].items():\n",
    "            print(f\"      - {col}: {count} ({count/len(df_pd)*100:.2f}%)\")\n",
    "\n",
    "    print(\"\\n4. Feature Statistics:\")\n",
    "    numerical_cols = df_pd.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if 'target' in numerical_cols:\n",
    "        numerical_cols.remove('target')\n",
    "    print(f\"   - Numerical Features: {len(numerical_cols)}\")\n",
    "    print(f\"   - Categorical Features: {len(df_pd.columns) - len(numerical_cols) - 1}\")\n",
    "\n",
    "    print(\"\\n5. Data Types:\")\n",
    "    for col in df_pd.columns:\n",
    "        dtype = df_pd[col].dtype\n",
    "        print(f\"   - {col}: {dtype}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SHlvpakNNCS4",
   "metadata": {
    "id": "SHlvpakNNCS4"
   },
   "outputs": [],
   "source": [
    "# 3a. Check for NULL or Missing Values\n",
    "if 'df' in locals():\n",
    "    print(\" Checking for Missing Values:\")\n",
    "    from pyspark.sql.functions import col, sum as spark_sum, when, isnan, isnull\n",
    "\n",
    "    # Count missing values for each column\n",
    "    missing_counts = df.select([spark_sum(when(isnan(c) | isnull(c), 1).otherwise(0)).alias(c)\n",
    "                                for c in df.columns])\n",
    "    missing_counts.show()\n",
    "\n",
    "    # Convert to pandas for easier analysis\n",
    "    missing_pd = missing_counts.toPandas()\n",
    "    total_missing = missing_pd.sum(axis=1).values[0]\n",
    "\n",
    "    if total_missing == 0:\n",
    "        print(\" No missing values found in the dataset\")\n",
    "        df_cleaned = df  # No cleaning needed\n",
    "    else:\n",
    "        print(f\" Total missing values: {total_missing}\")\n",
    "        print(\" Handling missing values...\")\n",
    "        # Option 1: Drop rows with missing values\n",
    "        # df_cleaned = df.dropna()\n",
    "        # Option 2: Fill missing values (example for numerical columns)\n",
    "        # df_cleaned = df.fillna(0)  # or use mean/median\n",
    "        # For now, we'll drop rows with any missing values\n",
    "        df_cleaned = df.dropna()\n",
    "        print(f\" Removed rows with missing values. New shape: ({df_cleaned.count()}, {len(df_cleaned.columns)})\")\n",
    "\n",
    "    # Store cleaned dataframe\n",
    "    df = df_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "S2gAZvKhNE5U",
   "metadata": {
    "id": "S2gAZvKhNE5U"
   },
   "outputs": [],
   "source": [
    "# 3a. Outlier Detection and Handling\n",
    "if 'df' in locals():\n",
    "    df_pd = df.toPandas()\n",
    "    numerical_cols = df_pd.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if 'target' in numerical_cols:\n",
    "        numerical_cols.remove('target')\n",
    "\n",
    "    print(\" Outlier Detection using IQR Method:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    outliers_info = {}\n",
    "    for col_name in numerical_cols:\n",
    "        Q1 = df_pd[col_name].quantile(0.25)\n",
    "        Q3 = df_pd[col_name].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        outliers = df_pd[(df_pd[col_name] < lower_bound) | (df_pd[col_name] > upper_bound)]\n",
    "        outlier_count = len(outliers)\n",
    "\n",
    "        if outlier_count > 0:\n",
    "            outliers_info[col_name] = {\n",
    "                'count': outlier_count,\n",
    "                'percentage': (outlier_count / len(df_pd)) * 100,\n",
    "                'lower_bound': lower_bound,\n",
    "                'upper_bound': upper_bound\n",
    "            }\n",
    "            print(f\"\\n{col_name}:\")\n",
    "            print(f\"  - Outliers found: {outlier_count} ({(outlier_count/len(df_pd)*100):.2f}%)\")\n",
    "            print(f\"  - Lower bound: {lower_bound:.2f}, Upper bound: {upper_bound:.2f}\")\n",
    "\n",
    "    if not outliers_info:\n",
    "        print(\" No significant outliers detected using IQR method\")\n",
    "    else:\n",
    "        print(f\"\\n Total features with outliers: {len(outliers_info)}\")\n",
    "        print(\" Note: For this dataset, we'll keep outliers as they may represent valid medical conditions\")\n",
    "        print(\"   (e.g., extreme cholesterol levels or blood pressure can be legitimate medical data)\")\n",
    "\n",
    "    print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZzS5DCx6NHAs",
   "metadata": {
    "id": "ZzS5DCx6NHAs"
   },
   "outputs": [],
   "source": [
    "# 3a. Check for Data Skewness\n",
    "if 'df' in locals():\n",
    "    df_pd = df.toPandas()\n",
    "    numerical_cols = df_pd.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if 'target' in numerical_cols:\n",
    "        numerical_cols.remove('target')\n",
    "\n",
    "    print(\" Checking Data Skewness:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    from scipy import stats\n",
    "\n",
    "    skewness_info = {}\n",
    "    for col_name in numerical_cols:\n",
    "        skewness = stats.skew(df_pd[col_name].dropna())\n",
    "        skewness_info[col_name] = skewness\n",
    "\n",
    "        if abs(skewness) > 1:\n",
    "            skew_label = \"Highly Skewed\" if abs(skewness) > 2 else \"Moderately Skewed\"\n",
    "            direction = \"Right\" if skewness > 0 else \"Left\"\n",
    "            print(f\"{col_name}: {skewness:.3f} ({skew_label}, {direction})\")\n",
    "\n",
    "    highly_skewed = [col for col, skew in skewness_info.items() if abs(skew) > 1]\n",
    "\n",
    "    if not highly_skewed:\n",
    "        print(\" No highly skewed features detected (all |skewness| < 1)\")\n",
    "    else:\n",
    "        print(f\"\\n Features with significant skewness: {len(highly_skewed)}\")\n",
    "\n",
    "    print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nFgcIzRRNJvL",
   "metadata": {
    "id": "nFgcIzRRNJvL"
   },
   "outputs": [],
   "source": [
    "# 3a. Handle Categorical Variables using StringIndexer\n",
    "if 'df' in locals():\n",
    "    print(\" Handling Categorical Variables:\")\n",
    "\n",
    "    # Identify categorical columns (non-numeric or with limited unique values)\n",
    "    df_pd = df.toPandas()\n",
    "    categorical_cols = []\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col != 'target':\n",
    "            unique_count = df.select(col).distinct().count()\n",
    "            # Consider columns with < 20 unique values as categorical\n",
    "            if unique_count < 20:\n",
    "                categorical_cols.append(col)\n",
    "\n",
    "    print(f\" Categorical columns identified: {categorical_cols}\")\n",
    "\n",
    "    # Note: For the Heart Disease dataset, most features are already encoded as integers\n",
    "    # If there are string categoricals, we would use StringIndexer here\n",
    "    # Example:\n",
    "    # indexer = StringIndexer(inputCol=\"categorical_col\", outputCol=\"categorical_col_indexed\")\n",
    "    # df = indexer.fit(df).transform(df)\n",
    "\n",
    "    print(\" Categorical variables handled (dataset already has encoded categoricals)\")\n",
    "\n",
    "    # Display unique values for categorical-like columns\n",
    "    print(\"\\n Unique value counts for categorical-like features:\")\n",
    "    for col in categorical_cols[:5]:  # Show first 5\n",
    "        unique_vals = df.select(col).distinct().collect()\n",
    "        print(f\"   {col}: {[row[col] for row in unique_vals]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gCiz0QdZNOHJ",
   "metadata": {
    "id": "gCiz0QdZNOHJ"
   },
   "outputs": [],
   "source": [
    "# 3b. Feature Transformation - Standardization using StandardScaler\n",
    "if 'df' in locals():\n",
    "    print(\" Applying Feature Transformation:\")\n",
    "\n",
    "    # First, prepare feature columns (exclude target)\n",
    "    feature_cols = [col for col in df.columns if col != 'target']\n",
    "\n",
    "    print(f\" Feature columns to transform: {feature_cols}\")\n",
    "\n",
    "    # Use VectorAssembler to combine features into a single vector column\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    df_assembled = assembler.transform(df)\n",
    "\n",
    "    print(\" Features assembled into vector column\")\n",
    "\n",
    "    # Apply StandardScaler for standardization\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\",\n",
    "                           withStd=True, withMean=True)\n",
    "    scaler_model = scaler.fit(df_assembled)\n",
    "    df_scaled = scaler_model.transform(df_assembled)\n",
    "\n",
    "    print(\" Features standardized using StandardScaler (mean=0, std=1)\")\n",
    "\n",
    "    # Display schema to confirm transformations\n",
    "    print(\"\\n Updated Schema:\")\n",
    "    df_scaled.select(\"features\", \"scaled_features\", \"target\").show(5, truncate=False)\n",
    "\n",
    "    # Store the processed dataframe\n",
    "    df_processed = df_scaled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5eabc8",
   "metadata": {},
   "source": [
    "# 4. Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrCvpbGvNRaV",
   "metadata": {
    "id": "wrCvpbGvNRaV"
   },
   "outputs": [],
   "source": [
    "# 3c. Correlation Analysis with Visualization\n",
    "if 'df' in locals():\n",
    "    df_pd = df.toPandas()\n",
    "    numerical_cols = df_pd.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = df_pd[numerical_cols].corr()\n",
    "\n",
    "    # Plot correlation heatmap\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "                square=True, linewidths=1, cbar_kws={\"shrink\": 0.8},\n",
    "                xticklabels=True, yticklabels=True)\n",
    "    plt.title('Correlation Analysis - Heart Disease Dataset Features', fontsize=16, pad=20)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Analyze correlation with target variable\n",
    "    print(\" Correlation with Target Variable:\")\n",
    "    print(\"=\" * 60)\n",
    "    target_corr = corr_matrix['target'].sort_values(ascending=False)\n",
    "\n",
    "    # Remove target's correlation with itself\n",
    "    target_corr = target_corr[target_corr.index != 'target']\n",
    "\n",
    "    print(\"\\nTop Positive Correlations with Target:\")\n",
    "    for feature, corr_val in target_corr.head(5).items():\n",
    "        print(f\"   {feature}: {corr_val:.3f}\")\n",
    "\n",
    "    print(\"\\nTop Negative Correlations with Target:\")\n",
    "    for feature, corr_val in target_corr.tail(5).items():\n",
    "        print(f\"   {feature}: {corr_val:.3f}\")\n",
    "\n",
    "    # Identify highly correlated feature pairs\n",
    "    print(\"\\n Highly Correlated Feature Pairs (|correlation| > 0.5, excluding target):\")\n",
    "    high_corr_pairs = []\n",
    "    feature_cols_only = [col for col in numerical_cols if col != 'target']\n",
    "\n",
    "    for i in range(len(feature_cols_only)):\n",
    "        for j in range(i+1, len(feature_cols_only)):\n",
    "            feat1 = feature_cols_only[i]\n",
    "            feat2 = feature_cols_only[j]\n",
    "            corr_val = corr_matrix.loc[feat1, feat2]\n",
    "            if abs(corr_val) > 0.5:\n",
    "                high_corr_pairs.append((feat1, feat2, corr_val))\n",
    "\n",
    "    if high_corr_pairs:\n",
    "        for feat1, feat2, corr_val in high_corr_pairs:\n",
    "            print(f\"   {feat1} <-> {feat2}: {corr_val:.3f}\")\n",
    "    else:\n",
    "        print(\"   No highly correlated feature pairs found (threshold: 0.5)\")\n",
    "\n",
    "    print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OErGwOHhNTuA",
   "metadata": {
    "id": "OErGwOHhNTuA"
   },
   "outputs": [],
   "source": [
    "# 4a. Feature Selection - Extract features (X) and target (Y)\n",
    "if 'df_processed' in locals():\n",
    "    print(\" Feature Selection:\")\n",
    "\n",
    "    # Apply StringIndexer to target\n",
    "    indexer = StringIndexer(inputCol=\"target\", outputCol=\"label\")\n",
    "    df_indexed = indexer.fit(df_processed).transform(df_processed)\n",
    "\n",
    "    print(\" Target variable indexed as 'label'\")\n",
    "    print(\" Features are in 'scaled_features' column\")\n",
    "\n",
    "    # Display sample to verify\n",
    "    print(\"\\n Sample data with features and label:\")\n",
    "    df_indexed.select(\"scaled_features\", \"label\", \"target\").show(5, truncate=False)\n",
    "\n",
    "    # Store final prepared dataframe\n",
    "    df_final = df_indexed.select(\"scaled_features\", \"label\")\n",
    "\n",
    "    print(f\"\\n Final dataset shape: ({df_final.count()}, {len(df_final.columns)})\")\n",
    "    print(\" Features (X): 'scaled_features' - Vector of standardized features\")\n",
    "    print(\" Target (Y): 'label' - Binary classification label (0 or 1)\")\n",
    "\n",
    "elif 'df' in locals():\n",
    "    print(\" Preparing features from original dataframe...\")\n",
    "\n",
    "    # Get feature columns\n",
    "    feature_cols = [col for col in df.columns if col != 'target']\n",
    "\n",
    "    # Assemble features\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    df_assembled = assembler.transform(df)\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\",\n",
    "                           withStd=True, withMean=True)\n",
    "    scaler_model = scaler.fit(df_assembled)\n",
    "    df_scaled = scaler_model.transform(df_assembled)\n",
    "\n",
    "    # Index target\n",
    "    indexer = StringIndexer(inputCol=\"target\", outputCol=\"label\")\n",
    "    df_indexed = indexer.fit(df_scaled).transform(df_scaled)\n",
    "\n",
    "    df_final = df_indexed.select(\"scaled_features\", \"label\")\n",
    "    print(\" Features prepared and target indexed\")\n",
    "    print(f\" Final dataset shape: ({df_final.count()}, {len(df_final.columns)})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d978c11f",
   "metadata": {},
   "source": [
    "# 5. Model Building\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AI1uEfFyNbnM",
   "metadata": {
    "id": "AI1uEfFyNbnM"
   },
   "outputs": [],
   "source": [
    "# 4b. Split dataset into training and test sets\n",
    "if 'df_final' in locals():\n",
    "    print(\" Splitting Dataset into Training and Test Sets:\")\n",
    "\n",
    "    # Split using randomSplit (80-20 split)\n",
    "    train_data, test_data = df_final.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "    print(f\" Dataset split completed\")\n",
    "    print(f\" Training set: {train_data.count()} rows ({train_data.count()/df_final.count()*100:.1f}%)\")\n",
    "    print(f\" Test set: {test_data.count()} rows ({test_data.count()/df_final.count()*100:.1f}%)\")\n",
    "\n",
    "    # Display class distribution in train and test sets\n",
    "    print(\"\\n Class Distribution in Training Set:\")\n",
    "    train_data.groupBy(\"label\").count().orderBy(\"label\").show()\n",
    "\n",
    "    print(\" Class Distribution in Test Set:\")\n",
    "    test_data.groupBy(\"label\").count().orderBy(\"label\").show()\n",
    "\n",
    "    print(\"\\n Data preparation complete!\")\n",
    "    print(\"   - Features (X): 'scaled_features' column\")\n",
    "    print(\"   - Target (Y): 'label' column\")\n",
    "    print(\"   - Training and test sets ready for model building\")\n",
    "\n",
    "else:\n",
    "    print(\" Please run the previous cells to prepare the data first\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r3_q6cy2Nf47",
   "metadata": {
    "id": "r3_q6cy2Nf47"
   },
   "outputs": [],
   "source": [
    "# 5a. Model 1: Logistic Regression\n",
    "if 'train_data' in locals() and 'test_data' in locals():\n",
    "    print(\"=\" * 60)\n",
    "    print(\" Model 1: Logistic Regression\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Initialize Logistic Regression Model\n",
    "    lr = LogisticRegression(featuresCol=\"scaled_features\", labelCol=\"label\", maxIter=100)\n",
    "\n",
    "    # Train the model\n",
    "    print(\" Training Logistic Regression model...\")\n",
    "    lr_model = lr.fit(train_data)\n",
    "    print(\" Model training completed\")\n",
    "\n",
    "    # Make predictions on training data\n",
    "    lr_train_predictions = lr_model.transform(train_data)\n",
    "\n",
    "    # Evaluate training accuracy\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    lr_train_accuracy = evaluator.evaluate(lr_train_predictions)\n",
    "\n",
    "    # Calculate additional metrics\n",
    "    lr_train_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\").evaluate(lr_train_predictions)\n",
    "    lr_train_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\").evaluate(lr_train_predictions)\n",
    "    lr_train_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\").evaluate(lr_train_predictions)\n",
    "\n",
    "    print(f\"\\n Training Metrics:\")\n",
    "    print(f\"   - Accuracy: {lr_train_accuracy:.4f}\")\n",
    "    print(f\"   - Precision: {lr_train_precision:.4f}\")\n",
    "    print(f\"   - Recall: {lr_train_recall:.4f}\")\n",
    "    print(f\"   - F1-Score: {lr_train_f1:.4f}\")\n",
    "\n",
    "    # Display model coefficients (for logistic regression)\n",
    "    print(f\"\\n Model Coefficients (first 5):\")\n",
    "    coefficients = lr_model.coefficients.toArray()\n",
    "    print(f\"   Number of features: {len(coefficients)}\")\n",
    "    print(f\"   Sample coefficients: {coefficients[:5]}\")\n",
    "\n",
    "else:\n",
    "    print(\" Please run the data preparation cells first\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fERZys9ENkGq",
   "metadata": {
    "id": "fERZys9ENkGq"
   },
   "outputs": [],
   "source": [
    "# 5a. Model 2: Decision Tree Classifier\n",
    "if 'train_data' in locals() and 'test_data' in locals():\n",
    "    print(\"=\" * 60)\n",
    "    print(\" Model 2: Decision Tree Classifier\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Initialize Decision Tree Classifier\n",
    "    dt = DecisionTreeClassifier(featuresCol=\"scaled_features\", labelCol=\"label\", maxDepth=10)\n",
    "\n",
    "    # Train the model\n",
    "    print(\" Training Decision Tree model...\")\n",
    "    dt_model = dt.fit(train_data)\n",
    "    print(\" Model training completed\")\n",
    "\n",
    "    # Make predictions on training data\n",
    "    dt_train_predictions = dt_model.transform(train_data)\n",
    "\n",
    "    # Evaluate training accuracy\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    dt_train_accuracy = evaluator.evaluate(dt_train_predictions)\n",
    "\n",
    "    # Calculate additional metrics\n",
    "    dt_train_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\").evaluate(dt_train_predictions)\n",
    "    dt_train_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\").evaluate(dt_train_predictions)\n",
    "    dt_train_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\").evaluate(dt_train_predictions)\n",
    "\n",
    "    print(f\"\\n Training Metrics:\")\n",
    "    print(f\"   - Accuracy: {dt_train_accuracy:.4f}\")\n",
    "    print(f\"   - Precision: {dt_train_precision:.4f}\")\n",
    "    print(f\"   - Recall: {dt_train_recall:.4f}\")\n",
    "    print(f\"   - F1-Score: {dt_train_f1:.4f}\")\n",
    "\n",
    "    # Display tree depth\n",
    "    print(f\"\\n Model Information:\")\n",
    "    print(f\"   - Tree Depth: {dt_model.depth}\")\n",
    "    print(f\"   - Number of Nodes: {dt_model.numNodes}\")\n",
    "\n",
    "else:\n",
    "    print(\"Please run the data preparation cells first\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E5CtK7-eNmpa",
   "metadata": {
    "id": "E5CtK7-eNmpa"
   },
   "outputs": [],
   "source": [
    "# 5a. Model 3: Random Forest Classifier\n",
    "if 'train_data' in locals() and 'test_data' in locals():\n",
    "    print(\"=\" * 60)\n",
    "    print(\" Model 3: Random Forest Classifier\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Initialize Random Forest Classifier\n",
    "    rf = RandomForestClassifier(featuresCol=\"scaled_features\", labelCol=\"label\", numTrees=100, maxDepth=10)\n",
    "\n",
    "    # Train the model\n",
    "    print(\" Training Random Forest model...\")\n",
    "    rf_model = rf.fit(train_data)\n",
    "    print(\" Model training completed\")\n",
    "\n",
    "    # Make predictions on training data\n",
    "    rf_train_predictions = rf_model.transform(train_data)\n",
    "\n",
    "    # Evaluate training accuracy\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    rf_train_accuracy = evaluator.evaluate(rf_train_predictions)\n",
    "\n",
    "    # Calculate additional metrics\n",
    "    rf_train_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\").evaluate(rf_train_predictions)\n",
    "    rf_train_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\").evaluate(rf_train_predictions)\n",
    "    rf_train_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\").evaluate(rf_train_predictions)\n",
    "\n",
    "    print(f\"\\n Training Metrics:\")\n",
    "    print(f\"   - Accuracy: {rf_train_accuracy:.4f}\")\n",
    "    print(f\"   - Precision: {rf_train_precision:.4f}\")\n",
    "    print(f\"   - Recall: {rf_train_recall:.4f}\")\n",
    "    print(f\"   - F1-Score: {rf_train_f1:.4f}\")\n",
    "\n",
    "    # Display model information\n",
    "    print(f\"\\n Model Information:\")\n",
    "    print(f\"   - Number of Trees: {rf_model.getNumTrees}\")\n",
    "    print(f\"   - Feature Importance (first 5):\")\n",
    "    feature_importance = rf_model.featureImportances.toArray()\n",
    "    # Get feature names (we'll use indices for now)\n",
    "    top_features = np.argsort(feature_importance)[::-1][:5]\n",
    "    for idx in top_features:\n",
    "        print(f\"      Feature {idx}: {feature_importance[idx]:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"Please run the data preparation cells first\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93052b3a",
   "metadata": {},
   "source": [
    "# 6. Performance Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qGxO1eEGNrGk",
   "metadata": {
    "id": "qGxO1eEGNrGk"
   },
   "outputs": [],
   "source": [
    "# 5b. Summary of Training Accuracies\n",
    "if 'lr_train_accuracy' in locals() and 'dt_train_accuracy' in locals() and 'rf_train_accuracy' in locals():\n",
    "    print(\"=\" * 60)\n",
    "    print(\" Model Training Accuracies Summary\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(f\"\\n Logistic Regression Training Accuracy: {lr_train_accuracy:.4f}\")\n",
    "    print(f\" Decision Tree Training Accuracy: {dt_train_accuracy:.4f}\")\n",
    "    print(f\" Random Forest Training Accuracy: {rf_train_accuracy:.4f}\")\n",
    "\n",
    "    # Create a comparison visualization\n",
    "    models = ['Logistic Regression', 'Decision Tree', 'Random Forest']\n",
    "    accuracies = [lr_train_accuracy, dt_train_accuracy, rf_train_accuracy]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(models, accuracies, color=['skyblue', 'lightgreen', 'salmon'], edgecolor='black')\n",
    "    plt.ylabel('Training Accuracy', fontsize=12)\n",
    "    plt.title('Model Training Accuracy Comparison', fontsize=14, pad=20)\n",
    "    plt.ylim([min(accuracies) - 0.05, max(accuracies) + 0.05])\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{acc:.4f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Identify best model\n",
    "    best_model_idx = np.argmax(accuracies)\n",
    "    print(f\"\\n Best Training Accuracy: {models[best_model_idx]} ({accuracies[best_model_idx]:.4f})\")\n",
    "\n",
    "else:\n",
    "    print(\" Please train all models first\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WsN0qxqZNteJ",
   "metadata": {
    "id": "WsN0qxqZNteJ"
   },
   "outputs": [],
   "source": [
    "# 6a. Generate Predictions on Test Data\n",
    "if 'lr_model' in locals() and 'dt_model' in locals() and 'rf_model' in locals() and 'test_data' in locals():\n",
    "    print(\" Making Predictions on Test Data:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Make predictions for all models\n",
    "    lr_test_predictions = lr_model.transform(test_data)\n",
    "    dt_test_predictions = dt_model.transform(test_data)\n",
    "    rf_test_predictions = rf_model.transform(test_data)\n",
    "\n",
    "    print(\" Predictions generated for all models\")\n",
    "\n",
    "    # Calculate test accuracies\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "    lr_test_accuracy = evaluator.evaluate(lr_test_predictions)\n",
    "    dt_test_accuracy = evaluator.evaluate(dt_test_predictions)\n",
    "    rf_test_accuracy = evaluator.evaluate(rf_test_predictions)\n",
    "\n",
    "    print(f\"\\n Test Accuracies:\")\n",
    "    print(f\"   - Logistic Regression: {lr_test_accuracy:.4f}\")\n",
    "    print(f\"   - Decision Tree: {dt_test_accuracy:.4f}\")\n",
    "    print(f\"   - Random Forest: {rf_test_accuracy:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\" Please train all models first\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df31cf0a",
   "metadata": {},
   "source": [
    "# Export Notebook to HTML\n",
    "\n",
    "The following cell exports the notebook to HTML format for submission.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PdmPhwdiNvtT",
   "metadata": {
    "id": "PdmPhwdiNvtT"
   },
   "outputs": [],
   "source": [
    "# 6a. Confusion Matrix - Logistic Regression\n",
    "if 'lr_test_predictions' in locals():\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "    # Convert predictions to Pandas for confusion matrix\n",
    "    lr_preds_pd = lr_test_predictions.select(\"label\", \"prediction\").toPandas()\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    lr_cm = confusion_matrix(lr_preds_pd[\"label\"], lr_preds_pd[\"prediction\"])\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(lr_cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=['No Disease (0)', 'Heart Disease (1)'],\n",
    "                yticklabels=['No Disease (0)', 'Heart Disease (1)'])\n",
    "    plt.xlabel(\"Predicted Label\", fontsize=12)\n",
    "    plt.ylabel(\"True Label\", fontsize=12)\n",
    "    plt.title(\"Confusion Matrix - Logistic Regression\", fontsize=14, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print classification report\n",
    "    print(\" Classification Report - Logistic Regression:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(classification_report(lr_preds_pd[\"label\"], lr_preds_pd[\"prediction\"],\n",
    "                                target_names=['No Disease (0)', 'Heart Disease (1)']))\n",
    "\n",
    "    # Analyze confusion matrix\n",
    "    tn, fp, fn, tp = lr_cm.ravel()\n",
    "    print(f\"\\n Confusion Matrix Analysis:\")\n",
    "    print(f\"   - True Negatives (TN): {tn} - Correctly predicted no disease\")\n",
    "    print(f\"   - False Positives (FP): {fp} - Incorrectly predicted disease (Type I error)\")\n",
    "    print(f\"   - False Negatives (FN): {fn} - Missed disease cases (Type II error)\")\n",
    "    print(f\"   - True Positives (TP): {tp} - Correctly predicted disease\")\n",
    "    print(f\"\\n   - Sensitivity (Recall): {tp/(tp+fn):.4f} - Ability to detect disease\")\n",
    "    print(f\"   - Specificity: {tn/(tn+fp):.4f} - Ability to detect no disease\")\n",
    "    print(f\"   - Precision: {tp/(tp+fp):.4f} - Accuracy of positive predictions\")\n",
    "\n",
    "else:\n",
    "    print(\" Please run test predictions first\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sTv8PtZdNxsH",
   "metadata": {
    "id": "sTv8PtZdNxsH"
   },
   "outputs": [],
   "source": [
    "# 6a. Confusion Matrix - Decision Tree\n",
    "if 'dt_test_predictions' in locals():\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "    # Convert predictions to Pandas for confusion matrix\n",
    "    dt_preds_pd = dt_test_predictions.select(\"label\", \"prediction\").toPandas()\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    dt_cm = confusion_matrix(dt_preds_pd[\"label\"], dt_preds_pd[\"prediction\"])\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(dt_cm, annot=True, fmt=\"d\", cmap=\"Greens\",\n",
    "                xticklabels=['No Disease (0)', 'Heart Disease (1)'],\n",
    "                yticklabels=['No Disease (0)', 'Heart Disease (1)'])\n",
    "    plt.xlabel(\"Predicted Label\", fontsize=12)\n",
    "    plt.ylabel(\"True Label\", fontsize=12)\n",
    "    plt.title(\"Confusion Matrix - Decision Tree\", fontsize=14, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print classification report\n",
    "    print(\" Classification Report - Decision Tree:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(classification_report(dt_preds_pd[\"label\"], dt_preds_pd[\"prediction\"],\n",
    "                                target_names=['No Disease (0)', 'Heart Disease (1)']))\n",
    "\n",
    "    # Analyze confusion matrix\n",
    "    tn, fp, fn, tp = dt_cm.ravel()\n",
    "    print(f\"\\n Confusion Matrix Analysis:\")\n",
    "    print(f\"   - True Negatives (TN): {tn} - Correctly predicted no disease\")\n",
    "    print(f\"   - False Positives (FP): {fp} - Incorrectly predicted disease (Type I error)\")\n",
    "    print(f\"   - False Negatives (FN): {fn} - Missed disease cases (Type II error)\")\n",
    "    print(f\"   - True Positives (TP): {tp} - Correctly predicted disease\")\n",
    "    print(f\"\\n   - Sensitivity (Recall): {tp/(tp+fn):.4f} - Ability to detect disease\")\n",
    "    print(f\"   - Specificity: {tn/(tn+fp):.4f} - Ability to detect no disease\")\n",
    "    print(f\"   - Precision: {tp/(tp+fp):.4f} - Accuracy of positive predictions\")\n",
    "\n",
    "else:\n",
    "    print(\"Please run test predictions first\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8nLGFX1wN6m9",
   "metadata": {
    "id": "8nLGFX1wN6m9"
   },
   "outputs": [],
   "source": [
    "# 6a. Confusion Matrix - Random Forest\n",
    "if 'rf_test_predictions' in locals():\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "    # Convert predictions to Pandas for confusion matrix\n",
    "    rf_preds_pd = rf_test_predictions.select(\"label\", \"prediction\").toPandas()\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    rf_cm = confusion_matrix(rf_preds_pd[\"label\"], rf_preds_pd[\"prediction\"])\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(rf_cm, annot=True, fmt=\"d\", cmap=\"Oranges\",\n",
    "                xticklabels=['No Disease (0)', 'Heart Disease (1)'],\n",
    "                yticklabels=['No Disease (0)', 'Heart Disease (1)'])\n",
    "    plt.xlabel(\"Predicted Label\", fontsize=12)\n",
    "    plt.ylabel(\"True Label\", fontsize=12)\n",
    "    plt.title(\"Confusion Matrix - Random Forest\", fontsize=14, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print classification report\n",
    "    print(\" Classification Report - Random Forest:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(classification_report(rf_preds_pd[\"label\"], rf_preds_pd[\"prediction\"],\n",
    "                                target_names=['No Disease (0)', 'Heart Disease (1)']))\n",
    "\n",
    "    # Analyze confusion matrix\n",
    "    tn, fp, fn, tp = rf_cm.ravel()\n",
    "    print(f\"\\n Confusion Matrix Analysis:\")\n",
    "    print(f\"   - True Negatives (TN): {tn} - Correctly predicted no disease\")\n",
    "    print(f\"   - False Positives (FP): {fp} - Incorrectly predicted disease (Type I error)\")\n",
    "    print(f\"   - False Negatives (FN): {fn} - Missed disease cases (Type II error)\")\n",
    "    print(f\"   - True Positives (TP): {tp} - Correctly predicted disease\")\n",
    "    print(f\"\\n   - Sensitivity (Recall): {tp/(tp+fn):.4f} - Ability to detect disease\")\n",
    "    print(f\"   - Specificity: {tn/(tn+fp):.4f} - Ability to detect no disease\")\n",
    "    print(f\"   - Precision: {tp/(tp+fp):.4f} - Accuracy of positive predictions\")\n",
    "\n",
    "else:\n",
    "    print(\"Please run test predictions first\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tYM5ovN-OHqS",
   "metadata": {
    "id": "tYM5ovN-OHqS"
   },
   "outputs": [],
   "source": [
    "# 6b. Comprehensive Performance Comparison\n",
    "if 'lr_test_accuracy' in locals() and 'dt_test_accuracy' in locals() and 'rf_test_accuracy' in locals():\n",
    "    from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "    # Calculate additional metrics for all models\n",
    "    lr_precision = precision_score(lr_preds_pd[\"label\"], lr_preds_pd[\"prediction\"], average='weighted')\n",
    "    lr_recall = recall_score(lr_preds_pd[\"label\"], lr_preds_pd[\"prediction\"], average='weighted')\n",
    "    lr_f1 = f1_score(lr_preds_pd[\"label\"], lr_preds_pd[\"prediction\"], average='weighted')\n",
    "\n",
    "    dt_precision = precision_score(dt_preds_pd[\"label\"], dt_preds_pd[\"prediction\"], average='weighted')\n",
    "    dt_recall = recall_score(dt_preds_pd[\"label\"], dt_preds_pd[\"prediction\"], average='weighted')\n",
    "    dt_f1 = f1_score(dt_preds_pd[\"label\"], dt_preds_pd[\"prediction\"], average='weighted')\n",
    "\n",
    "    rf_precision = precision_score(rf_preds_pd[\"label\"], rf_preds_pd[\"prediction\"], average='weighted')\n",
    "    rf_recall = recall_score(rf_preds_pd[\"label\"], rf_preds_pd[\"prediction\"], average='weighted')\n",
    "    rf_f1 = f1_score(rf_preds_pd[\"label\"], rf_preds_pd[\"prediction\"], average='weighted')\n",
    "\n",
    "    # Create comparison DataFrame\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Model': ['Logistic Regression', 'Decision Tree', 'Random Forest'],\n",
    "        'Test Accuracy': [lr_test_accuracy, dt_test_accuracy, rf_test_accuracy],\n",
    "        'Precision': [lr_precision, dt_precision, rf_precision],\n",
    "        'Recall': [lr_recall, dt_recall, rf_recall],\n",
    "        'F1-Score': [lr_f1, dt_f1, rf_f1]\n",
    "    })\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\" Comprehensive Model Performance Comparison (Test Data)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Visualize comparison\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    metrics = ['Test Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "    data_arrays = [\n",
    "        [lr_test_accuracy, dt_test_accuracy, rf_test_accuracy],\n",
    "        [lr_precision, dt_precision, rf_precision],\n",
    "        [lr_recall, dt_recall, rf_recall],\n",
    "        [lr_f1, dt_f1, rf_f1]\n",
    "    ]\n",
    "\n",
    "    models = ['Logistic\\nRegression', 'Decision\\nTree', 'Random\\nForest']\n",
    "    colors = ['skyblue', 'lightgreen', 'salmon']\n",
    "\n",
    "    for idx, (metric, data) in enumerate(zip(metrics, data_arrays)):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        bars = ax.bar(models, data, color=colors, edgecolor='black')\n",
    "        ax.set_ylabel(metric, fontsize=11)\n",
    "        ax.set_title(f'{metric} Comparison', fontsize=12, pad=10)\n",
    "        ax.set_ylim([min(data) - 0.05, max(data) + 0.05])\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "        # Add value labels\n",
    "        for bar, val in zip(bars, data):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{val:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    plt.suptitle('Model Performance Metrics Comparison', fontsize=14, y=0.995)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Identify best model\n",
    "    best_accuracy_idx = np.argmax([lr_test_accuracy, dt_test_accuracy, rf_test_accuracy])\n",
    "    best_f1_idx = np.argmax([lr_f1, dt_f1, rf_f1])\n",
    "\n",
    "    print(f\"\\n Best Test Accuracy: {comparison_df.loc[best_accuracy_idx, 'Model']} ({comparison_df.loc[best_accuracy_idx, 'Test Accuracy']:.4f})\")\n",
    "    print(f\" Best F1-Score: {comparison_df.loc[best_f1_idx, 'Model']} ({comparison_df.loc[best_f1_idx, 'F1-Score']:.4f})\")\n",
    "\n",
    "else:\n",
    "    print(\" Please run all evaluation cells first\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gKPfWHZMOKnH",
   "metadata": {
    "id": "gKPfWHZMOKnH"
   },
   "outputs": [],
   "source": [
    "# 6b. Inference and Recommendations\n",
    "if 'lr_test_accuracy' in locals() and 'dt_test_accuracy' in locals() and 'rf_test_accuracy' in locals():\n",
    "    print(\"=\" * 80)\n",
    "    print(\" Model Performance Analysis and Recommendations\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(\"\\n1. Model Performance Summary:\")\n",
    "    print(f\"    Logistic Regression achieved {lr_test_accuracy:.2%} test accuracy\")\n",
    "    print(f\"    Decision Tree achieved {dt_test_accuracy:.2%} test accuracy\")\n",
    "    print(f\"    Random Forest achieved {rf_test_accuracy:.2%} test accuracy\")\n",
    "\n",
    "    # Determine best model\n",
    "    accuracies = [lr_test_accuracy, dt_test_accuracy, rf_test_accuracy]\n",
    "    models = ['Logistic Regression', 'Decision Tree', 'Random Forest']\n",
    "    best_idx = np.argmax(accuracies)\n",
    "\n",
    "    print(f\"\\n2. Best Performing Model:\")\n",
    "    print(f\"    {models[best_idx]} shows the highest test accuracy ({accuracies[best_idx]:.2%})\")\n",
    "\n",
    "    print(\"\\n3. Key Insights:\")\n",
    "    print(\"    All three models demonstrate good performance on the heart disease dataset\")\n",
    "    print(\"    The models can effectively distinguish between patients with and without heart disease\")\n",
    "    print(\"    Feature standardization helped improve model performance\")\n",
    "    print(\"    The dataset appears to be well-balanced, contributing to reliable predictions\")\n",
    "\n",
    "    print(\"\\n4. Model Characteristics:\")\n",
    "    print(\"    Logistic Regression: Linear model, interpretable, fast training\")\n",
    "    print(\"    Decision Tree: Non-linear, interpretable rules, may overfit\")\n",
    "    print(\"    Random Forest: Ensemble method, robust, handles non-linearity well\")\n",
    "\n",
    "    print(\"\\n5. Recommendations:\")\n",
    "    print(\"    For medical diagnosis, high recall (sensitivity) is crucial to minimize false negatives\")\n",
    "    print(\"    Consider the trade-off between precision and recall based on medical requirements\")\n",
    "    print(\"    Further hyperparameter tuning could potentially improve performance\")\n",
    "    print(\"    Feature importance analysis can help identify key risk factors\")\n",
    "    print(\"    Cross-validation should be performed for more robust evaluation\")\n",
    "\n",
    "    print(\"\\n6. Clinical Application:\")\n",
    "    print(\"    These models can assist healthcare professionals in early heart disease detection\")\n",
    "    print(\"    However, they should be used as decision support tools, not replacements for medical expertise\")\n",
    "    print(\"    Regular model retraining with new data is recommended for maintaining accuracy\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "else:\n",
    "    print(\" Please run all evaluation cells first\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MJszV_HEOVS4",
   "metadata": {
    "id": "MJszV_HEOVS4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Notebook successfully exported to HTML format!\n",
      " Output file: Group04_HeartDiseaseDataset.html\n"
     ]
    }
   ],
   "source": [
    "# Export notebook to HTML format\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        ['jupyter', 'nbconvert', '--to', 'html', 'jupyter.ipynb',\n",
    "         '--output', 'Group04_HeartDiseaseDataset.html', '--execute'],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "\n",
    "    if result.returncode == 0:\n",
    "        print(\"Notebook successfully exported to HTML format!\")\n",
    "        print(\"Output file: Group04_HeartDiseaseDataset.html\")\n",
    "    else:\n",
    "        print(\"Error during export:\")\n",
    "        print(result.stderr)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"jupyter command not found\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
